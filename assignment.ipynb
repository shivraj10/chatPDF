{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#@title Install Dependencies\n",
        "!pip install langchain\n",
        "!pip install sentence_transformers\n",
        "!pip install xformers\n",
        "!pip install bitsandbytes accelerate transformers\n",
        "!pip install faiss-gpu\n",
        "!pip install accelerate\n",
        "!pip install pymupdf"
      ],
      "metadata": {
        "id": "W0p98F-FMq5s",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Import Libraries\n",
        "import io\n",
        "import fitz\n",
        "import torch\n",
        "import textwrap\n",
        "from google.colab import files\n",
        "from huggingface_hub import login\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from transformers import pipeline\n",
        "from langchain import HuggingFacePipeline, PromptTemplate\n",
        "from langchain.chains import RetrievalQA\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from transformers import BitsAndBytesConfig\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.schema.document import Document\n",
        "# Ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n"
      ],
      "metadata": {
        "id": "Wl3vrIAEPiKF",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Important functions\n",
        "def get_text(filename):\n",
        "\n",
        "    \"\"\"\n",
        "    Load pdf and extract the text\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    Filename : str\n",
        "        Path of the file uploaded\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    text : str\n",
        "        Extracted text form the pdf provided\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    doc = fitz.open(filename)\n",
        "    text_list = []\n",
        "    for page in doc:\n",
        "        text = page.get_text()\n",
        "        text_list.append(text)\n",
        "    text = ' '.join(text_list)\n",
        "    return text\n",
        "\n",
        "def authenticate():\n",
        "\n",
        "    \"\"\"\n",
        "    Using the Read and write token provided, allows to access the model from huggingface.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    #authentication token\n",
        "    access_token_read = \"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\"\n",
        "    access_token_write = \"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\"\n",
        "    login(token = access_token_read)\n",
        "\n",
        "def get_db(text):\n",
        "\n",
        "    \"\"\"\n",
        "    Creates a vector database from the provided string\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    text : str\n",
        "        String of characters\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    vectordb\n",
        "        Vector database of the string provided\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    embeddings = HuggingFaceEmbeddings()\n",
        "    text_splitter = CharacterTextSplitter(separator=\"\",chunk_size=1000, chunk_overlap=100)\n",
        "    docs = [Document(page_content=x) for x in text_splitter.split_text(text)]\n",
        "    vectordb = FAISS.from_documents(docs, embeddings)\n",
        "    return vectordb\n",
        "\n",
        "def initialize_llm_pipeline():\n",
        "\n",
        "    \"\"\"\n",
        "    Load and initiallize the LLAMA2-7B model and tokenizer, and create a pipeline using the same.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # Add quantization parameters\n",
        "    quantization_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_compute_dtype=torch.float16,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "    )\n",
        "\n",
        "    # Load Tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        \"NousResearch/Llama-2-7b-chat-hf\",\n",
        "        use_auth_token=True\n",
        "    )\n",
        "\n",
        "    # Load model\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        \"NousResearch/Llama-2-7b-chat-hf\",\n",
        "        device_map = 'auto',\n",
        "        offload_folder=\"save_folder\",\n",
        "        torch_dtype=torch.float32,\n",
        "        use_auth_token=True,\n",
        "        quantization_config=quantization_config,\n",
        "    )\n",
        "\n",
        "    # Create a pipeline using model and tokenizer\n",
        "    pipe = pipeline(\"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer= tokenizer,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        device_map=\"auto\",\n",
        "        max_new_tokens = 512,\n",
        "        do_sample=True,\n",
        "        top_k=30,\n",
        "        num_return_sequences=1,\n",
        "        eos_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    llm = HuggingFacePipeline(pipeline=pipe, model_kwargs={'temperature':0.1})\n",
        "\n",
        "\n",
        "    return llm\n",
        "\n",
        "def create_retrieval_chain(llm, vectordb):\n",
        "\n",
        "    \"\"\"\n",
        "    Create a retrieval chain using the pipeline and vector database\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    llm :\n",
        "        pipeline create from LLM model.\n",
        "\n",
        "    vectordb :\n",
        "        Vector database\n",
        "\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    qa_chain :\n",
        "        A question and answering retreival chain created using LLM pipeline and vector database.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # Prompt formate required for LLAMA2\n",
        "    SYSTEM_PROMPT = \"\"\"<<SYS>>\\n Use the following pieces of context to answer the question at the end.\n",
        "    If you don't know the answer, just say that you don't know, don't try to make up an answer. \\n<</SYS>>\\n\\n\"\"\"\n",
        "\n",
        "    INSTRUCTION = \"\"\"\n",
        "    {context}\n",
        "\n",
        "    Question: {question}\n",
        "    \"\"\"\n",
        "\n",
        "    # Prompt Template for the prompts\n",
        "    TEMPLATE = \"[INST]\" + SYSTEM_PROMPT + INSTRUCTION + \"[/INST]\"\n",
        "    prompt = PromptTemplate(template=TEMPLATE, input_variables=[\"context\", \"question\"])\n",
        "\n",
        "    # Create a retrieval chain\n",
        "    qa_chain = RetrievalQA.from_chain_type(\n",
        "        llm=llm,\n",
        "        chain_type=\"stuff\",\n",
        "        retriever=vectordb.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":3}),\n",
        "        return_source_documents=True,\n",
        "        chain_type_kwargs={\"prompt\": prompt},\n",
        "    )\n",
        "\n",
        "    return qa_chain"
      ],
      "metadata": {
        "id": "Fjb-YJc_OlRC",
        "cellView": "form"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Upload the file and Initialize models and vector database\n",
        "\n",
        "# Upload the file\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Get the filename\n",
        "filename = list(uploaded.keys())[0]\n",
        "\n",
        "# Get the text from PDF\n",
        "text = get_text(filename)\n",
        "\n",
        "# Authentication to access model from huggingface\n",
        "authenticate()\n",
        "\n",
        "# Store the text info in vector database using hugging face embeddings model\n",
        "vectordb = get_db(text)\n",
        "\n",
        "# Get the model and tokenizers in a pipeline\n",
        "llm_pipeline = initialize_llm_pipeline()\n",
        "\n",
        "# Get the Retreival Chain\n",
        "QA_chain = create_retrieval_chain(llm_pipeline, vectordb)"
      ],
      "metadata": {
        "id": "yfyy2ITWMn3P",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Enter Queries\n",
        "while 1:\n",
        "    query = str(input(\"Enter 'END' to exit the program\\nEnter your question! - \"))\n",
        "    if query == 'END':\n",
        "        break\n",
        "    result = QA_chain({\"query\": query})\n",
        "    print(f\"Answer - {textwrap.fill(result['result'], width=150)}\\n\")"
      ],
      "metadata": {
        "id": "_tZYZQ1APRzH",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}